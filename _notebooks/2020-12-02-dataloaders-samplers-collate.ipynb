{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# But what are PyTorch DataLoaders really?\n",
    "> Creating custom ways (without magic) to order, batch and combine your data with PyTorch DataLoaders.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- hide: false\n",
    "- image: images/copied_from_nb/my_icons/magichat.jpg\n",
    "- categories: [jupyter, visualisation, audio]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoaders are magic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can often get away with using something magical. You can bury your head in the sand and ignore the mysterious methods behind it, all while enjoying the benefits that come from this magic. But at some point, either curiousity will get the better of you or you'll be missing the flexibility you need, and you'll want to try to demystify the sorcery.\n",
    "\n",
    "In my opinion, the best libraries have an element of magic to them. They hide away some gory details with a little bit of polish and slight of hand that leave the world looking orderly and simple. The really great libraries allow you to peek behind the curtain at your own pace, slowly revealing the complexity and flexibility within.\n",
    "\n",
    "I believe PyTorch is one of those libraries. It has lots of composable abstractions that you can learn about independenlty which neatly layer together to make a powerful, customisable and elegant framework. In this tutorial, we're going to dive into some of the details of PyTorch `DataLoader`s in the hopes of discovering how it works behind the scenes and how we can customise it to our liking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.scottcondron.com/images/copied_from_nb/my_icons/magichat.jpg\" alt=\"Magic!\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's the plan?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch `DataLoader`s are great for iterating over batches of a `Dataset` like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "for xb, yb in dataloader:\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where `xb` and `yb` are _batches_ of your inputs and labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial is going to be about some of the more advanced features of `DataLoader`s which should explain what happens behind the scenes when you iterate over your dataloaders and help you customise different parts of that using PyTorch native features.\n",
    "\n",
    "To be specific, we're going to go over **custom collate functions** and [**Sampler**](https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Sampler)s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are `DataLoader`s and `Dataset`s?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial to be useful, you should probably know what `DataLoader`s and `Dataset`s are but I will refresh your memory. For a deeper dive, I recommend Jeremy Howard's tutorial [What is torch.nn _really_ ?](https://pytorch.org/tutorials/beginner/nn_tutorial.html) and the PyTorch docs [Writing Custom Datasets, DataLoaders and Transforms](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html).\n",
    "\n",
    "A quick refresher: PyTorch [Dataset](https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Dataset)s are just things that have a length and are indexable so that `len(dataset)` will work and `dataset[index]` will return a tuple of (x,y)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a little example that's mostly taken from [fastbook](https://github.com/fastai/fastbook) Chapter 4 to just quickly illustrate how simple a [Dataset](https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Dataset) is: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we'll create two lists for x and y values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xs values:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "ys values:  [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n"
     ]
    }
   ],
   "source": [
    "xs = list(range(10))\n",
    "ys = list(range(10,20))\n",
    "print('xs values: ', xs)\n",
    "print('ys values: ', ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use Python's `zip` function to combine them so `dataset[index]` returns `(x,y)` for that index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 10)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = list(zip(xs,ys))\n",
    "dataset[0] # returns the tuple (x[0], y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use `__getitem__` and `__len__`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also get the same functionality by using a class with the \"dunder/magic methods\" `__getitem__` (for `dataset[index]` functionality) and `__len__` (for `len(dataset)` functionality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse-show\n",
    "\n",
    "class MyDataset:\n",
    "    def __init__(self, xs, ys):\n",
    "        self.xs = xs\n",
    "        self.ys = ys\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.xs[i], self.ys[i]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 12)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = MyDataset(xs, ys)\n",
    "dataset[2] # returns the tuple (x[2], y[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Now use a `DataLoader`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we just wrap that in a [DataLoader](https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.DataLoader) and we can iterate it but _now_ they're magically `tensors` and we can use `DataLoader`s handy configurations like shuffling, batching, multi-processing, etc.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0]) tensor([10])\n",
      "tensor([1]) tensor([11])\n",
      "tensor([2]) tensor([12])\n",
      "tensor([3]) tensor([13])\n",
      "tensor([4]) tensor([14])\n",
      "tensor([5]) tensor([15])\n",
      "tensor([6]) tensor([16])\n",
      "tensor([7]) tensor([17])\n",
      "tensor([8]) tensor([18])\n",
      "tensor([9]) tensor([19])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "for x, y in DataLoader(dataset):\n",
    "    print(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the real fun is that we can get _batches_ of these by setting `batch_size`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1]) tensor([10, 11])\n",
      "tensor([2, 3]) tensor([12, 13])\n",
      "tensor([4, 5]) tensor([14, 15])\n",
      "tensor([6, 7]) tensor([16, 17])\n",
      "tensor([8, 9]) tensor([18, 19])\n"
     ]
    }
   ],
   "source": [
    "for x, y in DataLoader(dataset, batch_size=2):\n",
    "    print(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_And_ we can shuffle these batches by just setting `shuffle=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 5]) tensor([12, 15])\n",
      "tensor([7, 9]) tensor([17, 19])\n",
      "tensor([1, 4]) tensor([11, 14])\n",
      "tensor([0, 6]) tensor([10, 16])\n",
      "tensor([8, 3]) tensor([18, 13])\n"
     ]
    }
   ],
   "source": [
    "for x, y in DataLoader(dataset, batch_size=2, shuffle=True):\n",
    "    print(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it doesn't just shuffle the batches but instead, it shuffles the data and _then_ batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK... but can we customise this shuffling or batching??? Yes, we can customise the shuffling with a custom [**Sampler**](https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Sampler) and we can customise the batching with a **custom collate function**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Samplers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every `DataLoader` has a `Sampler` which is used internally to get the indices for each batch. Each index is used to index into your `Dataset` to grab the data (x, y). You can ignore this for now, but `DataLoader`s also have a `batch_sampler` which returns the indices for each batch in a list if `batch_size` is greater than 1. \n",
    "\n",
    "Don't worry if this is a bit confusing, it'll be more clear after a few examples hopefully:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the internal `.sampler` property of a few `DataLoader`s and see how it changes when the DataLoader configurations change:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SequentialSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When `shuffle=False`(default) with `batch_size=0`, the `sampler` returns each index in `0,1,2,3,4...` as you iterate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_sampler = DataLoader(dataset).sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when we iterate over the sampler we should get the indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for i in default_sampler:\n",
    "    #Â iterating over the SequentialSampler\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.sampler.SequentialSampler"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(default_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see it has a `sampler` property internally which is a `SequentialSampler`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import `SequentialSampler` to see if we can use it ourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.sampler import SequentialSampler\n",
    "\n",
    "sampler = SequentialSampler(dataset)\n",
    "\n",
    "for x in sampler:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, what about when `shuffle=True`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When shuffled, we should expect randomly shuffled indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "0\n",
      "7\n",
      "5\n",
      "2\n",
      "4\n",
      "6\n",
      "9\n",
      "8\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "random_sampler = DataLoader(dataset, shuffle=True).sampler\n",
    "for index in random_sampler:\n",
    "    print(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So `shuffle=True` changes the `sampler` internally, which returns random indices each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.sampler.RandomSampler"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(random_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see it's a `RandomSampler` so let's import that and use it ourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "7\n",
      "0\n",
      "3\n",
      "2\n",
      "4\n",
      "6\n",
      "5\n",
      "8\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.sampler import RandomSampler\n",
    "\n",
    "random_sampler = RandomSampler(dataset)\n",
    "\n",
    "for x in random_sampler:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pass this in explicitly to a `DataLoader` using the `sampler` parameter like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n",
      "0\n",
      "6\n",
      "8\n",
      "3\n",
      "9\n",
      "5\n",
      "7\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(dataset, sampler=random_sampler)\n",
    "for i in dl.sampler:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we've seen that every `DataLoader` has a `sampler` internally which is either `SequentialSampler` or `RandomSampler` depending on the value of `shuffle`, and these are iterated over to get the indices of the `Dataset` to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's great and all, but what if we want to customise the order of the data, other than shuffled or sequential. That's where custom `Sampler`s come in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the docs:\n",
    "> Every `Sampler` subclass has to provide an `__iter__` method, providing a way to iterate over indices of dataset elements, and a `__len__` method that returns the length of the returned iterators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So all we have to do to create a custom sampler is subclass `Sampler` and have a `__iter__` method (for iterating through the indices) and a `__len__` method for the length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a small toy example, say we wanted the **first half of the dataset to always happen first**, then the **second half to happen later in training** and we still to shuffle these two halfs independently:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: Making the order of the data less random is generally bad for training neural networks but let's forget about that for this example please."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "import random\n",
    "from torch.utils.data.sampler import Sampler\n",
    "\n",
    "class IndependentHalvesSampler(Sampler):\n",
    "    def __init__(self, dataset):\n",
    "        halfway_point = int(len(dataset)/2)\n",
    "        self.first_half_indices = list(range(halfway_point))\n",
    "        self.second_half_indices = list(range(halfway_point, len(dataset)))\n",
    "        \n",
    "    def __iter__(self):\n",
    "        random.shuffle(self.first_half_indices)\n",
    "        random.shuffle(self.second_half_indices)\n",
    "        return iter(self.first_half_indices + self.second_half_indices)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.first_half_indices) + len(self.second_half_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we've subclassed `Sampler`, we've stored the both halves of the indices in two lists and when `__iter__` is called (whenever the sampler is iterated over), it'll shuffle them independently and return an iterator of the two lists merged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First half indices:  [0, 1, 2, 3, 4]\n",
      "Second half indices: [5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "our_sampler = IndependentHalvesSampler(dataset)\n",
    "print('First half indices: ', our_sampler.first_half_indices)\n",
    "print('Second half indices:', our_sampler.second_half_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "4\n",
      "3\n",
      "0\n",
      "7\n",
      "9\n",
      "8\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "for i in our_sampler:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you can see that a shuffled [0,1,2,3,4] happen first, and then a shuffled [5, 6, 7, 8, 9] happen last."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can pass it to a `DataLoader` like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4]) tensor([14])\n",
      "tensor([0]) tensor([10])\n",
      "tensor([1]) tensor([11])\n",
      "tensor([3]) tensor([13])\n",
      "tensor([2]) tensor([12])\n",
      "tensor([7]) tensor([17])\n",
      "tensor([8]) tensor([18])\n",
      "tensor([5]) tensor([15])\n",
      "tensor([6]) tensor([16])\n",
      "tensor([9]) tensor([19])\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(dataset, sampler=our_sampler)\n",
    "for xb, yb in dl:\n",
    "    print(xb, yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we've seen what's responsible for the _order_ of the indices and we've seen how PyTorch uses `Sampler`s internally. We've also seen how to create our own `Sampler` subclass and pass it to PyTorch's `DataLoader`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A slight problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed a small problem above, if I make the batch size > half of the dataset, some indices in the two halves of the dataset will be appear in the same batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 2, 0, 3, 1, 5, 7]) tensor([14, 12, 10, 13, 11, 15, 17])\n",
      "tensor([8, 6, 9]) tensor([18, 16, 19])\n"
     ]
    }
   ],
   "source": [
    "batch_size=7\n",
    "dl = DataLoader(dataset, batch_size=batch_size, sampler=our_sampler)\n",
    "for xb, yb in dl:\n",
    "    print(xb, yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This goes against our original goal because we wanted the **first half of the dataset to always happen first**. Let's say we want all batches in the first half to be separate from the second half... that's where `batch_sampler`s come in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# BatchSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch #0 indices:  [0, 1, 2]\n",
      "Batch #1 indices:  [3, 4, 5]\n",
      "Batch #2 indices:  [6, 7, 8]\n",
      "Batch #3 indices:  [9]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 3\n",
    "default_batch_sampler = DataLoader(dataset, batch_size=batch_size).batch_sampler\n",
    "for i, batch_indices in enumerate(default_batch_sampler):\n",
    "    print(f'Batch #{i} indices: ', batch_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internally, PyTorch uses a `BatchSampler` to chunk together **the indices into batches**. We can make custom `Sampler`s which return _batches_ of indices and pass them using the `batch_sampler` argument. This is a bit more powerful in terms of customisation than `sampler` because **you can choose both the _order_ and the _batches_ at the same time**.\n",
    "\n",
    "For example, say for some reason you wanted to only batch certain things together (like only if they're the same length), or if you wanted to show some examples more often than other, a custom `BatchSampler` is great for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So rather than returning each index separately, the `batch_sampler` **iterates through batches of indices**. PyTorch uses the `sampler` internally to select the order, and the `batch_sampler` to batch together `batch_size` amount of indices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.sampler.BatchSampler"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(default_batch_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see it's a `BatchSampler` internally. Let's import this to see what it does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import BatchSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the `BatchSampler` docstring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wraps another sampler to yield a mini-batch of indices.\n",
      "\n",
      "    Args:\n",
      "        sampler (Sampler): Base sampler.\n",
      "        batch_size (int): Size of mini-batch.\n",
      "        drop_last (bool): If ``True``, the sampler will drop the last batch if\n",
      "            its size would be less than ``batch_size``\n",
      "\n",
      "    Example:\n",
      "        >>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=False))\n",
      "        [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]\n",
      "        >>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=True))\n",
      "        [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(BatchSampler.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you can initialise it with a `Sampler`, `batch_size` and `drop_last` (whether to remove the last batch), and it will return batches of indices when you iterate over it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch #0 indices:  [2, 4]\n",
      "Batch #1 indices:  [1, 3]\n",
      "Batch #2 indices:  [0, 9]\n",
      "Batch #3 indices:  [8, 6]\n",
      "Batch #4 indices:  [5, 7]\n"
     ]
    }
   ],
   "source": [
    "batch_sampler = BatchSampler(our_sampler, batch_size=2, drop_last=False)\n",
    "for i, batch_indices in enumerate(batch_sampler):\n",
    "    print(f'Batch #{i} indices: ', batch_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we can pass our custom `Sampler` to `BatchSampler` to control the order, and leave it responsible for batching the indices. We can then pass it to a `DataLoader` in the `batch_sampler` argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Batch Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to a custom `sampler`, you can also create a `batch_sampler`. Why? If for some reason you wanted to only batch certain things together (like only if they're the same length), or if you wanted to show some examples more often than others, a custom `BatchSampler` is great for this.\n",
    "\n",
    "To create a custom `batch_sampler`, we just do the same as we did with a custom `Sampler` but **our iterator returns batches of indices**, rather than individual indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a `BatchSampler` which only batches together values from the first half of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk(indices, chunk_size):\n",
    "    return torch.split(torch.tensor(indices), chunk_size)\n",
    "\n",
    "class EachHalfTogetherBatchSampler(Sampler):\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        halfway_point = len(dataset) // 2 \n",
    "        self.first_half_indices = list(range(halfway_point))\n",
    "        self.second_half_indices = list(range(halfway_point, len(dataset)))\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def __iter__(self):\n",
    "        random.shuffle(self.first_half_indices)\n",
    "        random.shuffle(self.second_half_indices)\n",
    "        first_half_batches  = chunk(self.first_half_indices, self.batch_size)\n",
    "        second_half_batches = chunk(self.second_half_indices, self.batch_size)\n",
    "        combined = list(first_half_batches + second_half_batches)\n",
    "        combined = [batch.tolist() for batch in combined]\n",
    "        random.shuffle(combined)\n",
    "        return iter(combined)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return (len(self.first_half_indices) + len(self.second_half_indices)) // self.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we've subclassed `Sampler`, we've stored the indices in two lists (as before) and when `__iter__` is called (whenever the `batch_sampler` is iterated over), it'll first batch them using a method we've called `chunk`. \n",
    "\n",
    "Then we merge the batches and finally, we shuffle the batches and return an iterator of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[5]\n",
      "[8, 6]\n",
      "[7, 9]\n",
      "[4, 2]\n",
      "[0, 3]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "each_half_together_batch_sampler = EachHalfTogetherBatchSampler(dataset, batch_size)\n",
    "for x in each_half_together_batch_sampler:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, as we hoped, none of the first and second half are batched together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And _now_, we can pass this to `DataLoader` using the `batch_sampler` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch #0. x0: tensor([7, 5, 8])\n",
      "          y0: tensor([17, 15, 18])\n",
      "Batch #1. x1: tensor([9, 6])\n",
      "          y1: tensor([19, 16])\n",
      "Batch #2. x2: tensor([2, 0])\n",
      "          y2: tensor([12, 10])\n",
      "Batch #3. x3: tensor([3, 1, 4])\n",
      "          y3: tensor([13, 11, 14])\n"
     ]
    }
   ],
   "source": [
    "for i, (xb,yb) in enumerate(DataLoader(dataset, batch_sampler=each_half_together_batch_sampler)):\n",
    "    print(f'Batch #{i}. x{i}:', xb)\n",
    "    print(f'          y{i}:', yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, great. That's how PyTorch chooses which elements in my `Dataset` to batch together... but where does that batching actually happen? And can we customise _that_ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Collate Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internally, PyTorch uses a **Collate Function** to combine the data in your batches together (*see note). \n",
    "By default, a function called [default_collate](https://github.com/pytorch/pytorch/blob/a03f05f2a28ee9516a26bab8e41351aeeb3a4d76/torch/utils/data/_utils/collate.py#L42) checks what type of data your `Dataset` returns and tries it's best to combine them data into a batch like a (x_batch, y_batch).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Note: For simplicity, we're going to assume automatic batching is enabled. See the PyTorch docs for details about collate functions when automatic batching is disabled: https://pytorch.org/docs/stable/data.html#torch.utils.data.Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But **what if we had custom types or multiple different types of data** which we wanted to handle which `default_collate` couldn't merge? We _could_ edit our `Dataset` so that they are mergable and that's solves some of the types issues **BUT** what if how we merged them depended on 'batch-level' information like the largest value in the batch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Note: For other fancy uses of custom collate functions, there's some cool examples in the popular [huggingface/transformers](https://github.com/huggingface/transformers/search?q=collate) library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For problems like these, custom collate functions are a handy way of solving them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the PyTorch docs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Users may use customized `collate_fn` to achieve custom batching, e.g., collating along a dimension other than the first, padding sequences of various lengths, or adding support for custom data types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input to your collate function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to match your custom collate function with the output of indexing your `Dataset`. If you dataset returns a tuple `(x, y)` when indexed into (like `dataset[0]`), then your collate function will need to take a list of tuples like `[(x0,y0), (x4,y4), (x2,y2)... ]` which is `batch_size` in length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing that custom collate functions are often used for is for padding variable length batches. So let's change our dataset so that each x is a list, and they're all different sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = list([torch.randint(0, 10, (x,)) for x in range(1, 11)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([5]),\n",
       " tensor([3, 8]),\n",
       " tensor([7, 7, 1]),\n",
       " tensor([2, 6, 5, 7]),\n",
       " tensor([5, 1, 4, 0, 7]),\n",
       " tensor([0, 1, 2, 1, 4, 9]),\n",
       " tensor([2, 3, 0, 9, 3, 4, 4]),\n",
       " tensor([2, 8, 8, 5, 7, 8, 2, 8]),\n",
       " tensor([5, 4, 0, 2, 1, 9, 5, 3, 2]),\n",
       " tensor([9, 2, 4, 7, 4, 3, 6, 6, 6, 7])]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 1, 4, 9]), 15)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = list(zip(xs,ys))\n",
    "dataset[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we try with the defaul collate function, it'll raise a `RuntimeError`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RuntimeError:  stack expects each tensor to be equal size, but got [1] at entry 0 and [2] at entry 1\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for xb, yb in DataLoader(dataset, batch_size=2):\n",
    "        print(xb)\n",
    "except RuntimeError as e:\n",
    "    print('RuntimeError: ', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With variable sized xs and a custom collate function, we could pad them to match the longest in the batch using `torch.nn.utils.rnn.pad_sequence`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def pad_x_collate_function(batch):\n",
    "    # batch looks like [(x0,y0), (x4,y4), (x2,y2)... ]\n",
    "    xs = [sample[0] for sample in batch]\n",
    "    ys = [sample[1] for sample in batch] \n",
    "    #Â If you want to be a little fancy, you can do the above in one line \n",
    "    #Â xs, ys = zip(*samples) \n",
    "    xs = pad_sequence(xs, batch_first=True, padding_value=0)\n",
    "    return xs, torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, we can pass this `pad_x_collate_function` to `collate_fn` in `DataLoader` and it will pad each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5, 0],\n",
      "        [3, 8]])\n",
      "tensor([[7, 7, 1, 0],\n",
      "        [2, 6, 5, 7]])\n",
      "tensor([[5, 1, 4, 0, 7, 0],\n",
      "        [0, 1, 2, 1, 4, 9]])\n",
      "tensor([[2, 3, 0, 9, 3, 4, 4, 0],\n",
      "        [2, 8, 8, 5, 7, 8, 2, 8]])\n",
      "tensor([[5, 4, 0, 2, 1, 9, 5, 3, 2, 0],\n",
      "        [9, 2, 4, 7, 4, 3, 6, 6, 6, 7]])\n"
     ]
    }
   ],
   "source": [
    "for xb, yb in DataLoader(dataset, batch_size=2, collate_fn=pad_x_collate_function):\n",
    "        print(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is it with `shuffle=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xs:  tensor([[9, 2, 4, 7, 4, 3, 6, 6, 6, 7],\n",
      "        [2, 8, 8, 5, 7, 8, 2, 8, 0, 0]])\n",
      "ys:  tensor([19, 17])\n",
      "xs:  tensor([[2, 6, 5, 7],\n",
      "        [5, 0, 0, 0]])\n",
      "ys:  tensor([13, 10])\n",
      "xs:  tensor([[5, 4, 0, 2, 1, 9, 5, 3, 2],\n",
      "        [2, 3, 0, 9, 3, 4, 4, 0, 0]])\n",
      "ys:  tensor([18, 16])\n",
      "xs:  tensor([[5, 1, 4, 0, 7],\n",
      "        [3, 8, 0, 0, 0]])\n",
      "ys:  tensor([14, 11])\n",
      "xs:  tensor([[7, 7, 1, 0, 0, 0],\n",
      "        [0, 1, 2, 1, 4, 9]])\n",
      "ys:  tensor([12, 15])\n"
     ]
    }
   ],
   "source": [
    "for xb, yb in DataLoader(dataset, shuffle=True, batch_size=2, collate_fn=pad_x_collate_function):\n",
    "    print('xs: ', xb)\n",
    "    print('ys: ', yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another slight problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there's a bit of an issue, some of the smaller values look the they have too much padding. Luckily, we've already created something that'll help here. We can use our `EachHalfTogetherBatchSampler` custom `batch_sampler` so that the first and second half are batched separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7, 7, 1, 0, 0],\n",
      "        [5, 1, 4, 0, 7]])\n",
      "tensor([[5, 4, 0, 2, 1, 9, 5, 3, 2],\n",
      "        [0, 1, 2, 1, 4, 9, 0, 0, 0]])\n",
      "tensor([[2, 3, 0, 9, 3, 4, 4, 0],\n",
      "        [2, 8, 8, 5, 7, 8, 2, 8]])\n",
      "tensor([[2, 6, 5, 7],\n",
      "        [5, 0, 0, 0]])\n",
      "tensor([[9, 2, 4, 7, 4, 3, 6, 6, 6, 7]])\n",
      "tensor([[3, 8]])\n"
     ]
    }
   ],
   "source": [
    "each_half_together_batch_sampler = EachHalfTogetherBatchSampler(dataset, batch_size=2)\n",
    "for xb, yb in DataLoader(dataset, collate_fn=pad_x_collate_function, batch_sampler=each_half_together_batch_sampler):\n",
    "    print(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there we go, you can see the zero padding (but not too much!) at the end of each batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I recommend you to run for this yourself and create your own your Samplers and collate functions. All the code from this post is available [on Github](https://github.com/scottire/fastpages/tree/master/_notebooks/2020-11-11-dataloaders-samplers-collate.ipynb).\n",
    "\n",
    "I personally love learning about new parts of PyTorch and finding ways to interact with them. What do you think about these styles of explorations? Did you learn a bit about DataLoaders, Samplers and collate functions by reading this article? If so, feel free to share it, and youâ€™re also more than welcome to contact me (via [Twitter](https://twitter.com/_ScottCondron)) if you have any questions, comments, or feedback.\n",
    "\n",
    "Thanks for reading! :rocket:\n",
    "\n",
    "[Follow me on Twitter here](https://www.twitter.com/_scottcondron) for more stuff like this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://twitter.com/_ScottCondron?ref_src=twsrc%5Etfw\" class=\"twitter-follow-button\" data-show-count=\"false\">Follow @_ScottCondron</a><script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
